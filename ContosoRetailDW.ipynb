{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "// Declare the values for your Azure SQL DB\nval jdbcUsername = \"USERNAME\"\nval jdbcPassword = \"PASSWORD\"\nval jdbcHostname = \"URL.database.windows.net\"\nval jdbcPort = 1433\nval jdbcDatabase = \"ContosoRetail\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1517881491727_0004</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-huffer.zblltpn25yre5gdrwdpgajzsnc.gx.internal.cloudapp.net:8088/proxy/application_1517881491727_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.9:30060/node/containerlogs/container_1517881491727_0004_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\njdbcDatabase: String = ContosoRetail"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "import java.util.Properties\n\nval jdbc_url = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=60;\"\nval connectionProperties = new Properties()\nconnectionProperties.put(\"user\",s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\",s\"${jdbcPassword}\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res4: Object = null"}], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "val sqlTableDF = spark.read.jdbc(jdbc_url, \"ContosoRetail.dbo.FactInventory\", connectionProperties)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "sqlTableDF: org.apache.spark.sql.DataFrame = [InventoryKey: int, DateKey: timestamp ... 14 more fields]"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "sqlTableDF.printSchema", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- InventoryKey: integer (nullable = false)\n |-- DateKey: timestamp (nullable = false)\n |-- StoreKey: integer (nullable = false)\n |-- ProductKey: integer (nullable = false)\n |-- CurrencyKey: integer (nullable = false)\n |-- OnHandQuantity: integer (nullable = false)\n |-- OnOrderQuantity: integer (nullable = false)\n |-- SafetyStockQuantity: integer (nullable = true)\n |-- UnitCost: decimal(19,4) (nullable = false)\n |-- DaysInStock: integer (nullable = true)\n |-- MinDayInStock: integer (nullable = true)\n |-- MaxDayInStock: integer (nullable = true)\n |-- Aging: integer (nullable = true)\n |-- ETLLoadID: integer (nullable = true)\n |-- LoadDate: timestamp (nullable = true)\n |-- UpdateDate: timestamp (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "sqlTableDF.show(1)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error was encountered:\nSession 0 did not reach idle status in time. Current status is busy.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "sqlTableDF.select(\"OnHandQuantity\",\"UnitCost\").show(1)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error was encountered:\nSession 0 did not reach idle status in time. Current status is busy.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "// extracts schema data in the .CSV and loads the data from .CSV in a dataframe\n\nval userSchema = spark.read.option(\"header\", \"true\").csv(\"wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\").schema\nval readDf = spark.read.format(\"csv\").schema(userSchema).load(\"wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "readDf: org.apache.spark.sql.DataFrame = [Date: string, Time: string ... 5 more fields]"}], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "// readDf creates a temp table, then use temp table to create a hive table\n\nreadDf.createOrReplaceTempView(\"temphvactable\")\nspark.sql(\"create table hvactable_hive as select * from temphvactable\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res8: org.apache.spark.sql.DataFrame = []"}], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "// use the hive table to create a table in Azure SQL DB\nspark.table(\"hvactable_hive\").write.jdbc(jdbc_url, \"hvactable\", connectionProperties)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 13, "cell_type": "code", "source": "// packages for streaming into a hvac table\n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.streaming._\nimport java.sql.{Connection,DriverManager,ResultSet}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import java.sql.{Connection, DriverManager, ResultSet}"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "// Stream data from .CSV into the hvac table\n// Get the schema of the data to be streamed\n// Create a streaming dataframe using that schema.\n\nval userSchema = spark.read.option(\"header\", \"true\").csv(\"wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\").schema\nval readStreamDf = spark.readStream.schema(userSchema).csv(\"wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/\") \nreadStreamDf.printSchema", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- Date: string (nullable = true)\n |-- Time: string (nullable = true)\n |-- TargetTemp: string (nullable = true)\n |-- ActualTemp: string (nullable = true)\n |-- System: string (nullable = true)\n |-- SystemAge: string (nullable = true)\n |-- BuildingID: string (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "val WriteToSQLQuery = readStreamDf.writeStream.foreach(new ForeachWriter[Row] {\n    var connection:java.sql.Connection = _\n    var statement:java.sql.Statement = _\n    \n    val jdbcUserName = \"USERNAME\"\n    val jdbcPassword = \"PASSWORD\"\n    val jdbcHostname = \"URL.database.windows.net\"\n    val jdbcPort = 1433\n    val jdbcDatabase = \"ContosoRetail\"\n    val driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n    val jdbc_url = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n    \n    def open(partitionId: Long, version: Long):Boolean = {\n        Class.forName(driver)\n        connection = DriverManager.getConnection(jdbc_url, jdbcUsername, jdbcPassword)\n        statement = connection.createStatement\n        true\n    }\n    \n    def process(value: Row): Unit = {\n        val Date = value(0)\n        val Time = value(1)\n        val TargetTemp = value(2)\n        val ActualTemp = value(3)\n        val System = value(4)\n        val SystemAge = value(5)\n        val BuildingID = value(6)\n        \n        val valueStr = \"'\" + Date + \"'\" + Time + \"'\" + TargetTemp + \"'\" + ActualTemp + \"'\" + System + \"'\" + SystemAge + \"'\" + BuildingID + \"'\"\n        statement.execute(\"INSERT INTO \" + \"dbo.hvactable\" + \" VALUES (\" + valueStr + \")\")\n        }\n    \n    def close(errorOrNull: Throwable): Unit = {\n        connection.close\n    }\n})\n\nvar steamingQuery = WriteToSQLQuery.start()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "steamingQuery: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@4346c691"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}